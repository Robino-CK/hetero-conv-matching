{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /storage/scratch/0994359/hetero-conv-matching/.venv/lib/python3.9/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /storage/scratch/0994359/hetero-conv-matching/.venv/lib/python3.9/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "from Coarsener.HeteroRGCNCoarsener import HeteroRGCNCoarsener\n",
    "from Data.DBLP import DBLP\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch_geometric\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = DBLP() \n",
    "original_graph = dataset.load_graph(n_components=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/dgl/transforms/functional.py:1366: DGLWarning: share_ndata argument has been renamed to copy_ndata.\n",
      "  dgl_warning(\"share_ndata argument has been renamed to copy_ndata.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authortopaper\n",
      "conferencetopaper\n",
      "papertoauthor\n",
      "papertoconference\n",
      "papertoterm\n",
      "termtopaper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/scratch/0994359/hetero-conv-matching/Coarsener/HeteroCoarsener.py:375: UserWarning: torch.searchsorted(): boundary tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous boundary tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:39.)\n",
      "  indices = torch.searchsorted(sorted_mapping[:, 0], query_nodes)\n"
     ]
    }
   ],
   "source": [
    "device= \"cuda:3\"\n",
    "#original_graph = original_graph.to(device)\n",
    "num_nearest_init_neighbors_per_type = {\"papertoauthor\": 10, \"authortopaper\": 10, \"conferencetopaper\":10, \"papertoconference\":10,\"papertoterm\":10, \"termtopaper\":10, \"author\":10 , \"paper\":10, \"conference\": 10, \"term\":20}\n",
    "\n",
    "coarsener = HeteroRGCNCoarsener(original_graph, 0.1, num_nearest_init_neighbors_per_type, device=device, use_cca=True, pairs_per_level=20,norm_p=1, approx_neigh=True, add_feat=False, use_out_degree=False) \n",
    "\n",
    "coarsener.init()\n",
    "coarsener.r = 0.5\n",
    "coarsener.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio 0.9899724433557869\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "coarsend_graph = coarsener.summarized_graph\n",
    "\n",
    "coarsend_graph = coarsend_graph.cpu()\n",
    "mapping = coarsener.get_mapping(\"author\")\n",
    "coarsener.make_mask(mapping, \"author\")\n",
    "\n",
    "labels = coarsener.get_labels(mapping, \"author\")\n",
    "coarsend_graph.nodes[\"author\"].data[\"label\"] = torch.tensor([labels[i] for i in range(len(labels)) ],  device=coarsend_graph.device) #,\n",
    "print(\"ratio\", coarsend_graph.num_nodes()/ original_graph.num_nodes() ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import HeteroData\n",
    "def from_dgl_hetero_manual(g: dgl.DGLHeteroGraph) -> HeteroData:\n",
    "    \"\"\"\n",
    "    Convert a DGL heterogeneous graph into a PyG HeteroData object.\n",
    "\n",
    "    Args:\n",
    "        g (dgl.DGLHeteroGraph): Input DGL heterogeneous graph with node/edge features stored in ndata/edata.\n",
    "\n",
    "    Returns:\n",
    "        HeteroData: A PyG HeteroData object with the same node and edge features.\n",
    "    \"\"\"\n",
    "    data = HeteroData()\n",
    "\n",
    "    # Convert node features for each node type\n",
    "    for ntype in g.ntypes:\n",
    "        # Collect all feature fields for this node type\n",
    "        ndata = g.nodes[ntype].data\n",
    "        # If there is a 'feat' entry, map it to .x in PyG\n",
    "        if 'feat' in ndata:\n",
    "            data[ntype].feat = ndata['feat']\n",
    "        # Map any other node data fields directly\n",
    "        for key, value in ndata.items():\n",
    "            if key != 'feat':\n",
    "                data[ntype][key] = value\n",
    "\n",
    "    # Convert edges for each canonical edge type\n",
    "    for c_etype in g.canonical_etypes:\n",
    "        src_type, etype, dst_type = c_etype\n",
    "        # Get edge index (source, target)\n",
    "        src_nodes, dst_nodes = g.edges(etype=c_etype)\n",
    "        edge_index = torch.stack([src_nodes, dst_nodes], dim=0)\n",
    "        # Assign edge_index\n",
    "        data[c_etype].edge_index = edge_index\n",
    "\n",
    "        # Collect edge features\n",
    "        edata = g.edges[c_etype].data\n",
    "        # If there is a 'feat' entry, map it to .edge_attr\n",
    "        if 'feat' in edata:\n",
    "            data[c_etype].edge_attr = edata['feat']\n",
    "        # Map any other edge data fields directly\n",
    "        for key, value in edata.items():\n",
    "            if key != 'feat':\n",
    "                data[c_etype][key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "def dgl_to_pyg_input(g, device=\"cpu\"):\n",
    "    # Convert DGL heterograph to PyG's HeteroData\n",
    "    pyg_data = from_dgl_hetero_manual  (g)\n",
    "\n",
    "    x_dict = {}\n",
    "    for ntype in g.ntypes:\n",
    "        if 'feat' in g.nodes[ntype].data:\n",
    "            x_dict[ntype] = g.nodes[ntype].data['feat'].to(device)\n",
    "\n",
    "    edge_index_dict = {}\n",
    "    for canonical_etype in g.canonical_etypes:\n",
    "        src_type, rel_type, dst_type = canonical_etype\n",
    "        src, dst = g.edges(etype=canonical_etype)\n",
    "        edge_index = torch.stack([src, dst], dim=0)\n",
    "        edge_index_dict[(src_type, rel_type, dst_type)] = edge_index.to(device)\n",
    "    pyg_data.edge_index_dict = edge_index_dict\n",
    "    return pyg_data,x_dict, edge_index_dict, g.ntypes, g.canonical_etypes\n",
    "\n",
    "original_data, o_x_dict, o_edge_index_dict, o_node_types, o_edge_types = dgl_to_pyg_input(original_graph, )\n",
    "coarsened_data, c_x_dict, c_edge_index_dict, c_node_types, c_edge_types = dgl_to_pyg_input(coarsend_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mapping_author_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dst_all_orig \u001b[38;5;241m=\u001b[39m original_graph\u001b[38;5;241m.\u001b[39medges(etype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthortopaper\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m src_orig, dst_orig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(src_all_orig, dst_all_orig):\n\u001b[0;32m----> 4\u001b[0m     src_c \u001b[38;5;241m=\u001b[39m \u001b[43mmapping_author_array\u001b[49m[src_orig\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[1;32m      5\u001b[0m     dst_c \u001b[38;5;241m=\u001b[39m mapping_paper_array[dst_orig\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[1;32m      6\u001b[0m     edges \u001b[38;5;241m=\u001b[39m coarsend_graph\u001b[38;5;241m.\u001b[39medge_ids(src_c, dst_c, etype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthortopaper\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mapping_author_array' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "\n",
    "class ImprovedHeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, x_dict,num_classes, target_feat=\"author\", num_layers=2, dropout=0.3, with_non_linear = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.target_feat = target_feat\n",
    "        # Extract node types and edge types from metadata\n",
    "        node_types, edge_types = metadata[0], metadata[1]\n",
    "        \n",
    "        # Create embedding layers for each node type with proper dimensions\n",
    "        self.embeddings = torch.nn.ModuleDict()\n",
    "        for node_type, feat_dim in { i: x_dict[i].size(1) for i in  x_dict.keys()}.items():\n",
    "            self.embeddings[node_type] = torch.nn.Linear(feat_dim, hidden_channels)\n",
    "        \n",
    "        # Multiple heterogeneous conv layers for message passing\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv_dict = {}\n",
    "            for edge_type in edge_types:\n",
    "                # Use the proper dimensions for source and target nodes\n",
    "                conv_dict[edge_type] = SAGEConv(\n",
    "                    hidden_channels, \n",
    "                    hidden_channels, \n",
    "                 #   add_self_loops=False,\n",
    "                    normalize=True\n",
    "                )\n",
    "            self.convs.append(HeteroConv(conv_dict, aggr='mean'))\n",
    "        \n",
    "        # Layer normalization for each node type\n",
    "        self.layer_norms = torch.nn.ModuleDict({\n",
    "            node_type: torch.nn.LayerNorm(hidden_channels)\n",
    "            for node_type in node_types\n",
    "        })\n",
    "        if with_non_linear:\n",
    "            # Output projection layers\n",
    "            self.output_projs = torch.nn.ModuleDict({\n",
    "                node_type: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Dropout(dropout),\n",
    "                    torch.nn.Linear(hidden_channels, num_classes if node_type == target_feat else hidden_channels)\n",
    "                )\n",
    "                for node_type in node_types\n",
    "            })\n",
    "        else:\n",
    "            # Output projection layers\n",
    "            self.output_projs = torch.nn.ModuleDict({\n",
    "                node_type: torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "                    torch.nn.Dropout(dropout),\n",
    "                    torch.nn.Linear(hidden_channels, num_classes if node_type == target_feat else hidden_channels)\n",
    "                )\n",
    "                for node_type in node_types\n",
    "            })\n",
    "                \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Initial embedding of node features\n",
    "        x_dict = {node_type: self.embeddings[node_type](x) \n",
    "                 for node_type, x in x_dict.items()}\n",
    "        \n",
    "        # Apply multiple layers of heterogeneous graph convolutions\n",
    "        for conv in self.convs:\n",
    "            # Store previous embeddings for residual connections\n",
    "            x_dict_prev = {k: v.clone() for k, v in x_dict.items()}\n",
    "            \n",
    "            # Apply heterogeneous convolution\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            \n",
    "            # Apply layer normalization, non-linearity, dropout and residual connection\n",
    "            x_dict = {\n",
    "                node_type: self.layer_norms[node_type](\n",
    "                    F.relu(x) + x_dict_prev[node_type]  # Residual connection\n",
    "                )\n",
    "                for node_type, x in x_dict.items()\n",
    "            }\n",
    "            \n",
    "            # Apply dropout to intermediate representations\n",
    "            x_dict = {\n",
    "                node_type: F.dropout(x, p=self.dropout, training=self.training)\n",
    "                for node_type, x in x_dict.items()\n",
    "            }\n",
    "        \n",
    "        # Final projection for each node type\n",
    "        output_dict = {\n",
    "            node_type: self.output_projs[node_type](x)\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "        \n",
    "        # Apply log softmax to author nodes (for classification)\n",
    "        if self.target_feat in output_dict:\n",
    "            output_dict[self.target_feat] = F.log_softmax(output_dict[self.target_feat], dim=1)\n",
    "            \n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_data.to(\"cpu\")\n",
    "coarsened_data =coarsened_data.to(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Original Graph - Loss: 1.3887, Accuracy: 0.3000\n",
      "  Coarsened Graph - Loss: 1.3910, Accuracy: 0.2877\n",
      "  Inverted Coarsened - Accuracy: 0.0000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# Train both models\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     original_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_original\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     coarsened_loss \u001b[38;5;241m=\u001b[39m train_coarsened()\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 85\u001b[0m, in \u001b[0;36mtrain_original\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(\n\u001b[1;32m     81\u001b[0m     out[target_node_type][original_data[target_node_type]\u001b[38;5;241m.\u001b[39mtrain_mask], \n\u001b[1;32m     82\u001b[0m     original_data[target_node_type][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m][original_data[target_node_type]\u001b[38;5;241m.\u001b[39mtrain_mask]\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     84\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 85\u001b[0m \u001b[43moptimizer_original\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/0994359/hetero-conv-matching/.venv/lib64/python3.9/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries (if not already imported)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "#from Models.GNNs.HGCN import ImprovedHeteroGNN\n",
    "\n",
    "\n",
    "original_x_dict = o_x_dict\n",
    "coarsened_x_dict = c_x_dict\n",
    "o_metadata = (o_node_types, o_edge_types)\n",
    "c_metadata=(c_node_types, c_edge_types)\n",
    "#original_x_dict.update({\"conference\": torch.zeros((original_data[\"conference\"][\"num_nodes\"], 1))})# = 0  #= 0#\n",
    "#coarsened_x_dict.update({\"conference\": torch.zeros((coarsened_data[\"conference\"][\"num_nodes\"], 1))})# = 0  #= 0#\n",
    "\n",
    "#num_classes = len(original_data[\"author\"][\"label\"].unique())\n",
    "device = \"cpu\"\n",
    "target_node_type = \"author\"\n",
    "# Your existing model definition\n",
    "model_original = ImprovedHeteroGNN(metadata=o_metadata,target_feat=target_node_type, x_dict= original_x_dict ,num_classes= 4, dropout=0.1, hidden_channels=256, with_non_linear= True)\n",
    "model_coarsened = ImprovedHeteroGNN(metadata=c_metadata, target_feat=target_node_type,x_dict= coarsened_x_dict,num_classes= 4, dropout=0.1,hidden_channels=256, with_non_linear= True)\n",
    "model_original = model_original.to(device)\n",
    "model_coarsened = model_coarsened.to(device)\n",
    "# Optimizers\n",
    "optimizer_original = torch.optim.Adam(model_original.parameters(), lr=0.0001) #, weight_decay=5e-4 \n",
    "optimizer_coarsened = torch.optim.Adam(model_coarsened.parameters(), lr=0.0001) #, weight_decay=5e-4\n",
    "\n",
    "\n",
    "# Function to apply inversion mapping from coarsened to original\n",
    "def apply_inversion_mapping(coarsened_pred, inversion_map, original_labels, test_mask):\n",
    "    \"\"\"\n",
    "    Maps predictions from coarsened graph back to original graph nodes\n",
    "    \n",
    "    Args:\n",
    "        coarsened_pred: Predictions on coarsened graph\n",
    "        inversion_map: Mapping from coarsened nodes to original nodes\n",
    "        original_labels: Ground truth labels of original graph\n",
    "        \n",
    "    Returns:\n",
    "        mapped_accuracy: Accuracy after applying inversion mapping\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for orig_node, coarsened_node in inversion_map.items():\n",
    "        if not test_mask[coarsened_node]:\n",
    "            continue\n",
    "        coarse_pred = coarsened_pred[coarsened_node]\n",
    "        orig_label = original_labels[orig_node]\n",
    "        #print(coarse_pred, orig_label)\n",
    "\n",
    "        correct = (coarse_pred == orig_label)\n",
    "    \n",
    "                \n",
    "        if correct:\n",
    "            correct_count += 1\n",
    "     #   else:\n",
    "      #      print(f\"Mismatch: Coarsened Node {coarsened_node} predicted {coarse_pred}, Original Node {orig_node} label {orig_label}\")\n",
    "        total_count += 1\n",
    "    \n",
    "    return correct_count / total_count if total_count > 0 else 0\n",
    "def apply_inversion_mapping_new(coarsened_pred, inversion_map, original_labels, test_mask):\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    for orig_node, coarsened_node in inversion_map.items():\n",
    "        if coarsened_node not in test_mask:\n",
    "            continue\n",
    "        coarse_pred = coarsened_pred[coarsened_node]\n",
    "        orig_label = original_labels[orig_node]\n",
    "        if coarse_pred == orig_label:\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "    return correct_count / total_count if total_count > 0 else 0\n",
    "# Training function for original graph\n",
    "def train_original():\n",
    "    model_original.train()\n",
    "    optimizer_original.zero_grad()\n",
    "    \n",
    "    #print(original_x_original_data.edge_index_dict)\n",
    "    out = model_original(original_x_dict, original_data.edge_index_dict)\n",
    "    loss = F.nll_loss(\n",
    "        out[target_node_type][original_data[target_node_type].train_mask], \n",
    "        original_data[target_node_type][\"label\"][original_data[target_node_type].train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer_original.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Training function for coarsened graph\n",
    "def train_coarsened():\n",
    "    model_coarsened.train()\n",
    "    optimizer_coarsened.zero_grad()\n",
    "    out = model_coarsened(coarsened_x_dict, coarsened_data.edge_index_dict)\n",
    "    loss = F.nll_loss(\n",
    "        out[target_node_type][coarsened_data[target_node_type].train_mask], \n",
    "        coarsened_data[target_node_type][\"label\"][coarsened_data[target_node_type].train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer_coarsened.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function for original graph\n",
    "def test_original():\n",
    "    model_original.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model_original(original_x_dict, original_data.edge_index_dict)\n",
    "        pred = out[target_node_type].argmax(dim=1)\n",
    "        \n",
    "        # Calculate accuracy on test set\n",
    "        correct = pred[original_data[target_node_type].test_mask] == original_data[target_node_type][\"label\"][original_data[target_node_type].test_mask]\n",
    "        acc = int(correct.sum()) / int(original_data[target_node_type].test_mask.sum())\n",
    "        return acc\n",
    "\n",
    "# Evaluation function for coarsened graph\n",
    "def test_coarsened():\n",
    "    model_coarsened.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model_coarsened(original_x_dict, original_data.edge_index_dict)\n",
    "        pred = out[target_node_type].argmax(dim=1)\n",
    "        \n",
    "        # Calculate accuracy on test set\n",
    "        correct = pred[original_data[target_node_type].test_mask] == original_data[target_node_type][\"label\"][original_data[target_node_type].test_mask]\n",
    "        acc = int(correct.sum()) / int(original_data[target_node_type].test_mask.sum())\n",
    "        return acc, 0\n",
    "    \n",
    "\n",
    "# Training for 50 epochs and comparing both models\n",
    "results = {\n",
    "    \"epoch\": [],\n",
    "    \"original_loss\": [],\n",
    "    \"coarsened_loss\": [],\n",
    "    \"original_acc\": [],\n",
    "    \"coarsened_acc\": [],\n",
    "    \"inverted_acc\": []\n",
    "}\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # Train both models\n",
    "    original_loss = train_original()\n",
    "    coarsened_loss = train_coarsened()\n",
    "    \n",
    "    # Store results\n",
    "    results[\"epoch\"].append(epoch+1)\n",
    "    results[\"original_loss\"].append(original_loss)\n",
    "    results[\"coarsened_loss\"].append(coarsened_loss)\n",
    "    \n",
    "    # Evaluate every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        original_acc = test_original()\n",
    "        coarsened_acc, inverted_acc = test_coarsened()\n",
    "        \n",
    "        results[\"original_acc\"].append(original_acc)\n",
    "        results[\"coarsened_acc\"].append(coarsened_acc)\n",
    "        results[\"inverted_acc\"].append(inverted_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "    \n",
    "    \n",
    "        print(f'  Original Graph - Loss: {original_loss:.4f}, Accuracy: {original_acc:.4f}')\n",
    "        print(f'  Coarsened Graph - Loss: {coarsened_loss:.4f}, Accuracy: {coarsened_acc:.4f}')\n",
    "        print(f'  Inverted Coarsened - Accuracy: {inverted_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "# Visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results[\"epoch\"], results[\"original_loss\"], label=\"Original Graph\")\n",
    "plt.plot(results[\"epoch\"], results[\"coarsened_loss\"], label=\"Coarsened Graph\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "x = [results[\"epoch\"][i] for i in range(0, epochs, 10)]\n",
    "plt.plot(x, results[\"original_acc\"], 'o-', label=\"Original Graph\")\n",
    "plt.plot(x, results[\"coarsened_acc\"], 's-', label=\"Coarsened Graph\")\n",
    "#plt.plot(x, results[\"inverted_acc\"], '^-', label=\"Inverted Coarsened\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comparison\n",
    "print(\"Final Performance Comparison:\")\n",
    "print(f\"Original Graph Accuracy: {results['original_acc'][-1]:.4f}\")\n",
    "print(f\"Coarsened Graph Accuracy: {results['coarsened_acc'][-1]:.4f}\")\n",
    "#print(f\"Inverted Coarsened Accuracy: {results['inverted_acc'][-1]:.4f}\")\n",
    "\n",
    "# Calculate speedup from using coarsened graph\n",
    "# (You would need to time the training for a proper comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_all_orig = original_graph.edges(etype=\"authortopaper\")[0]\n",
    "dst_all_orig = original_graph.edges(etype=\"authortopaper\")[1]\n",
    "for src_orig, dst_orig in zip(src_all_orig, dst_all_orig):\n",
    "    src_c = mapping_author_array[src_orig.item()]\n",
    "    dst_c = mapping_paper_array[dst_orig.item()]\n",
    "    edges = coarsend_graph.edge_ids(src_c, dst_c, etype=\"authortopaper\")\n",
    "    assert len(edges) != 0, \"warning\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
